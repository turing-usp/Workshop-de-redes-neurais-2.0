{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Redes Neurais\n",
    "## Turing USP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pytorch logo](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/nelson/miniconda3/envs/py38/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: torchvision in /home/nelson/miniconda3/envs/py38/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: typing-extensions in /home/nelson/miniconda3/envs/py38/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/nelson/miniconda3/envs/py38/lib/python3.8/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/nelson/miniconda3/envs/py38/lib/python3.8/site-packages (from torchvision) (1.21.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision # Se voc√™ n√£o os tiver no seu computador, pode levar um tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• B√°sicos de Pytorch üî•\n",
    "\n",
    "### Tensores\n",
    "*Os blocos de constru√ß√£o das redes neurais*\n",
    "\n",
    "Primeiro vamos ver alguns an√°logos entre **numpy** e **Pytorch**\n",
    "\n",
    "#### Matrizes\n",
    " - Em Pytorch, matrizes (*arrays*) s√£o chamados de tensores.\n",
    " - Uma matriz $3\\times3$, por exemplo √© um tensor $3\\times3$\n",
    " - Podemos criar um array numpy com o m√©todo `np.array()`\n",
    " - Podemos pegar o tipo do array com `type()`\n",
    " - Podemos pegar o formato do *array* com `np.shape()`. Linha $\\times$ Coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array do tipo: <class 'numpy.ndarray'>\n",
      "Array de formato: (2, 3)\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "primeiro_array = np.array(array) # array 2x3\n",
    "print(f\"Array do tipo: {type(primeiro_array)}\")\n",
    "print(f\"Array de formato: {np.shape(primeiro_array)}\")\n",
    "print(primeiro_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criamos um tensor com o m√©todo `torch.Tensor()`\n",
    "- `tensor.type`: tipo do objeto, nesse caso um tensor\n",
    "- `tensor.shape`: formato do tensor. Linha $\\times$ Coluna \n",
    "- `tensor.device` : por onde este tensor est√° sendo processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: <built-in method type of Tensor object at 0x7fd11d7b3e00>\n",
      "Tensor de formato: torch.Size([2, 3])\n",
      "Tensor sendo armazenado em cpu\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.Tensor(array)\n",
    "print(f\"Tipo: {tensor.type}\")\n",
    "print(f\"Tensor de formato: {tensor.shape}\")\n",
    "print(f\"Tensor sendo armazenado em {tensor.device}\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer a aloca√ß√£o de *arrays* de maneira an√°loga nas duas linguagens:\n",
    " - `np.ones()` = `torch.ones()`\n",
    " - `np.random.rand()` = `torch.rand()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy:\\n {np.ones((2,3))}\\n\")\n",
    "\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:\n",
      " [[0.22411498 0.71031727 0.76547858]\n",
      " [0.98029385 0.60100558 0.87637373]]\n",
      "\n",
      "tensor([[0.4204, 0.7241, 0.0798],\n",
      "        [0.8919, 0.9841, 0.4038]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy:\\n {np.random.rand(2,3)}\\n\")\n",
    "\n",
    "print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo de numpy para torch e vice-versa\n",
    "\n",
    "Em muitos pontos **numpy** e **pytorch** s√£o bem parecidos em suas estruturas, e muitas das vezes podemos utilizar os dois em conjunto. Assim normalmente convertemos resultados de redes neurais - que s√£o tensores - para **arrays** de **numpy**.\n",
    "\n",
    "Os m√©todos para fazer a convers√£o entre tensores e arrays numpy:\n",
    " - `torch.from_numpy()`: de um array numpy para um tensor\n",
    " - `tensor.numpy()`: de um tensor para um array numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " [[0.08500939 0.06881937]\n",
      " [0.41455516 0.0145194 ]] \n",
      "\n",
      "tensor([[0.0850, 0.0688],\n",
      "        [0.4146, 0.0145]], dtype=torch.float64) \n",
      "\n",
      "<class 'numpy.ndarray'> \n",
      " [[0.08500939 0.06881937]\n",
      " [0.41455516 0.0145194 ]]\n"
     ]
    }
   ],
   "source": [
    "array = np.random.rand(2,2)\n",
    "print(f\"{type(array)} \\n {array} \\n\")\n",
    "\n",
    "de_numpy_para_tensor = torch.from_numpy(array)\n",
    "print(f\"{de_numpy_para_tensor} \\n\")\n",
    "\n",
    "tensor = de_numpy_para_tensor\n",
    "de_tensor_para_numpy = tensor.numpy()\n",
    "print(f\"{type(de_tensor_para_numpy)} \\n {de_tensor_para_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando fazemos estas convers√µes tamb√©m podemos fazer um *typecast* (mudan√ßa do tipo) das vari√°veis, isso pode ser √∫til j√° que o Pytorch faz uma s√©rie de computa√ß√µes de baixo n√≠vel, o qual o tipo primitivo das vari√°veis precisa ser bem especificado e definido, para isso podemos usar o m√©todo `tensor.type(torch.TipoDeTensor)`, alguns tipo de tensores nativos do Pytorch s√£o:\n",
    "  - `torch.FloatTensor` - pontos flutuantes de 32-bits\n",
    "  - `torch.DoubleTensor` - pontos flutuantes de 64-bits\n",
    "  - `torch.IntTensor` - n√∫meros inteiros de 32-bits\n",
    "  - `torch.LongTensor` - n√∫meros inteiros de 64-bits\n",
    "√â muito comum encontrarmos *bugs* causados pela utiliza√ß√£o errada de algum tipo primitivo, voc√™ pode ler sobre todos eles na [documenta√ß√£o do Pytorch](https://pytorch.org/docs/stable/tensors.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> \n",
      " tensor([[ 1., 10.],\n",
      "        [ 2., 20.]])\n",
      "\n",
      "<class 'torch.Tensor'> \n",
      " tensor([[ 1, 10],\n",
      "        [ 2, 20]])\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,10],[2,20]])\n",
    "\n",
    "# Transformar em um tensor de Floats:\n",
    "tensor_float = torch.from_numpy(array).type(torch.FloatTensor)\n",
    "print(f\"{type(tensor_float)} \\n {tensor_float}\\n\")\n",
    "\n",
    "# Transformar em um tensor de Longs:\n",
    "tensor_long = torch.from_numpy(array).type(torch.LongTensor)\n",
    "print(f\"{type(tensor_long)} \\n {tensor_long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opera√ß√µes com tensores\n",
    "Existem mais de 100 opera√ß√µes implementadas para tensores, incluindo aritm√©tica, √°lgebra linear, manipula√ß√£o de matrizes etc. √â interessante que voc√™ as cheque [aqui](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "O mais interessante, inclusive algo que possibilitou a utiliza√ß√£o em massa de redes neurais, √© o processamento dessas opera√ß√µes em GPU's (que geralmente possuem uma maior velocidade do que CPU's).\n",
    "\n",
    "Por padr√£o tensores s√£o criados na CPU. N√≥s podemos explicitamente mover para GPU's utilizando o m√©todo `.to` (isso, claro, se voc√™ pode usar uma GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para esse notebook podemos usar a GPU? False\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "print(f\"Para esse notebook podemos usar a GPU? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Move nosso tensor para uma GPU se poss√≠vel\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opera√ß√µes de splicing padr√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira linha:  tensor([1., 1., 1.])\n",
      "Primeira coluna:  tensor([1., 1., 1.])\n",
      "√öltima coluna: tensor([1., 1., 1.])\n",
      "tensor([[1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.]])\n",
      "torch.Size([9]): tensor([1., 0., 1., 1., 0., 1., 1., 0., 1.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Primeira linha: ', tensor[0])\n",
    "\n",
    "print('Primeira coluna: ', tensor[:, 0])\n",
    "\n",
    "print('√öltima coluna:', tensor[..., -1])\n",
    "\n",
    "tensor[:,1] = 0 # Colocar a segunda coluna como 0's\n",
    "print(tensor)\n",
    "\n",
    "print(f\"{tensor.view(9).shape}: {tensor.view(9)} \\n\") # Podemos mudar o formato do tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Opera√ß√µes aritm√©ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adi√ß√£o: \n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) \n",
      "\n",
      "Subtra√ß√£o: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n",
      "Multiplica√ß√£o elemento-a-elemento: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Divis√£o elemento-a-elemento: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Multiplica√ß√£o de matriz:\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "\n",
    "print(f\"Adi√ß√£o: \\n{torch.add(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Subtra√ß√£o: \\n{torch.sub(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Multiplica√ß√£o elemento-a-elemento: \\n{torch.mul(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Divis√£o elemento-a-elemento: \\n{torch.div(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Multiplica√ß√£o de matriz:\\n{torch.matmul(tensor, tensor.T)}\\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opera√ß√µes com s√≥ um tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma: 15.0\n",
      "\n",
      "M√©dia: 3.0 \n",
      "\n",
      "Desvio padr√£o: 1.5811388492584229 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "\n",
    "print(f\"Soma: {tensor.sum()}\\n\")\n",
    "\n",
    "print(f\"M√©dia: {tensor.mean()} \\n\")\n",
    "\n",
    "print(f\"Desvio padr√£o: {tensor.std()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd e back propagation\n",
    "*Diferencia√ß√£o autom√°tica e back prop com `torch.autograd`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando treinamos redes neurais, o algoritmo mais usado √© a back propagation. Nesse algoritmo, par√¢metros (os *weights* do modelo) s√£o ajustados de acordo com o gradiente da fun√ß√£o de perda em respeito com o par√¢metro dado.\n",
    "\n",
    "Para computar esses gradientes, o PyTorch tem uma implementa√ß√£o de diferencia√ß√£o (o c√°lculo de derivadas) chamado `torch.autograd`. Ele faz computa√ß√µes autom√°ticas de gradientes para qualquer *computational graph*.\n",
    "\n",
    "Considere a rede neural mais simples de uma camada, com entrada `x`, par√¢metros `w` e `b` e alguma fun√ß√£o de perda. Ela pode ser definida da seguinte maneira:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama do computational graph](https://i.imgur.com/x6DBPFQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # [1 1 1 1 1] vetor de entrada\n",
    "\n",
    "y = torch.ones(3)*2  # [2 2 2] valor esperado\n",
    "\n",
    "w = torch.full((5, 3), 3.0, requires_grad=True) # [ 3 3 3 ; 3 3 3 ; 3 3 3 ; 3 3 3 ; 3 3 3] matriz de pesos\n",
    "\n",
    "b = torch.ones(3, requires_grad=True) # [1 1 1] matriz de bias\n",
    "\n",
    "z = torch.matmul(x, w) + b # [16 16 16] \n",
    "\n",
    "loss = torch.sum(torch.pow(z,y)) # [768] fun√ß√£o de perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos computar os gradientes seguindo esse diagrama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagrama para calcular os gradiantes dos par√¢metros](https://i.imgur.com/fSoQQBC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para otimizar os pesos (weights) dos par√¢metros da rede neural, precisamos computar as derivadas da nossa \"fun√ß√£o de perda\" em respeito aos par√¢metros. Precisamente $\\frac{\\partial \\, \\mathrm{loss}}{\\partial w}$ e $\\frac{\\partial \\, \\mathrm{loss}}{\\partial b}$ para valores fixos de `x` e `y`. Para computar as derivadas, utilizamos `loss.backward()` os valores ficam armazenados em `w.grad` e `b.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32., 32., 32.],\n",
      "        [32., 32., 32.],\n",
      "        [32., 32., 32.],\n",
      "        [32., 32., 32.],\n",
      "        [32., 32., 32.]])\n",
      "tensor([32., 32., 32.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por padr√£o, todos os tensores com par√¢metro `requires_grad=True` est√£o monitorando seu hist√≥rico de fun√ß√µes computadas para calcular seu gradiente. Por√©m em alguns casos isso pode n√£o ser necess√°rio, isso pode acontecer em casos como:\n",
    " - Para marcar alguns par√¢metros como **frozen parameters**. Algo comum quando voc√™ quer aperfei√ßoar uma rede pr√©-treinada\n",
    " - Para **acelearar** as computa√ß√µes quando voc√™ est√° apenas passando pelo passo de **forward**, no qual computa√ß√µes com tensores que n√£o monitoram gradientes s√£o mais √∫teis.\n",
    "\n",
    "Podemos para de monitorar os gradientes colocando nosso c√≥digo em um bloco com  `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Monitora os gradientes\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "# N√£o monitora os gradientes\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo resultado com o m√©todo `detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets e Dataloader\n",
    "*Um meio de padronizar e otimizar dados para as redes neurais no PyTorch*\n",
    "\n",
    "O PyTorch fornece dois \"tipos primitivos\" (√© como se fossem ints, floats, bools) para otimizar e padronizar datasets e depois dizer para a rede neural como ela deve ler esse dataset, eles s√£o o `torch.utils.data.Dataset` e o `torch.utils.data.DataLoader`.\n",
    "\n",
    "O PyTorch tamb√©m nos fornece alguns datasets j√° prontos, o [Fashion-MINIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/) √© um deles, ele √© um dataset com imagens de roupas em 28x28 com 60.000 imagens de treino e 10.000 de teste. Vamos usar ele tanto nesse exemplo quanto no exemplo de redes neurais.\n",
    "\n",
    "Mesmo que o dataset j√° esteja montado, vamos passar quais seriam os passos para criar um dataset do zero. Vale dizer que nem sempre precisamos realizar esses passos, muitas vezes podemos passar nossos dados de maneira \"cru\" como tensores, mas √© interessante sab√™-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset # estrutura de dataset de tensores \n",
    "from torchvision import datasets # datasets j√° existentes no pytorch\n",
    "from torchvision.transforms import ToTensor # para transformar as imagens em tensores\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = 'data', # Onde vai armazenar o dataset\n",
    "    train = True, # Especifica que √© o dataset de treino\n",
    "    download= True, # baixa o dataset da internet\n",
    "    transform= ToTensor() # transforma em tensor\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    download= True, \n",
    "    transform= ToTensor() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos dar uma olhada em como s√£o as imagens do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando seu dataset para o treino com o Dataloader\n",
    "\n",
    "Quando voc√™ treinar seu modelo com seus dados do dataset, voc√™ vai querer pass√°-los como _minibatches_ (quantos dados ser√£o alimentados antes de treinar o modelo), e tamb√©m embaralhar esses minibathcs a cada _epoch_ (uma passada por todo o dataset) para que o modelo n√£o veja informa√ß√£o na sequ√™ncia dos dados. O **Dataloader** √© a ferramenta do PyTorch que n√≥s possibilita fazer isso de maneira facilitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) # Aqui definimos o tamanho do batch como 64 e que embaralhe a amostra\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato do batch das features: torch.Size([64, 1, 28, 28])\n",
      "Formato do batch das labels: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Dress\n"
     ]
    }
   ],
   "source": [
    "# Podemos visualizar uma batch do nosso dataloader\n",
    "train_features, train_labels = next(iter(train_dataloader)) # O dataloader √© um objeto iter√°vel\n",
    "print(f\"Formato do batch das features: {train_features.size()}\") # repare como s√£o 64 imagens de 28x28 pixels\n",
    "print(f\"Formato do batch das labels: {train_labels.size()}\") # repare como s√£o 64 labels\n",
    "img = train_features[0].squeeze() # remove todas as dimens√µes com valores 1\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {labels_map[label.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando um dataset customizado\n",
    "Como tinha dito antes, normalmente n√£o teremos um dataset bonitinho assim na natureza, e normalmente voc√™ ter√° que o fazer. Muitas vezes, pode-se usar apenas tensores, sem criar o dataset e o dataloader, por√©m essa geralmente n√£o √© a op√ß√£o mais padronizada nem mais optimizada para imagens e textos. Segue um exemplo de como construir seu pr√≥prio dataset:\n",
    "\n",
    "Toda classe customizada de dataset deve conter 3 m√©todos: \n",
    "- `__init__` : M√©todo que √© executado quando voc√™ inst√¢ncia (cria/chama) o dataset. Normalmente voc√™ vai passar o endere√ßo dos seus dados, de suas categorias e se precisar, alguma transforma√ß√£o neles.\n",
    "- `__len__` : Serve para falar quantas amostras existem no seu dataset\n",
    "- `__getitem__` : Serve quando voc√™ tem que pegar um elemento do seu dataset dado um √≠ndice `idx`. Vai identificar o endere√ßo do elemento no disco e converter para um tensor e aplicar as transforma√ß√µes, caso voc√™ as tenha solicitado. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset para dados de \"tabela\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X = X_data\n",
    "        self.y = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√£o vai rodar pq n√£o temos o dataset\n",
    "# train_data = CustomDataset(torch.FloatTensor(X_train),torch.FloatTensor(y_train.values))\n",
    "# test_data = CustomDataset(torch.FloatTensor(X_test),torch.FloatTensor(y_test.values))\n",
    "\n",
    "# train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "# test_loader=DataLoader(test_data,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dataframes simples, voc√™ tamb√©m pode pular a parte de criar a classe e usar um `TensorDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√£o vai rodar pq n√£o temos o dataset\n",
    "# Pegar as colunas do dataframe\n",
    "# target_train = df['Target'].values\n",
    "# features_train = df.drop('Target', axis=1).values\n",
    "\n",
    "# target_validation = df_validation['Target'].values\n",
    "# features_validation = df_validation.drop('Target', axis=1).values\n",
    "\n",
    "# Transformar em dataset de tensores\n",
    "# treino = torch.utils.data.TensorDataset(features_train, target_train)\n",
    "# validacao = torch.utils.data.TensorDataset(features_validation, target_validation)\n",
    "\n",
    "# Preparamos o dataset para ser iterado pela rede\n",
    "# treino_loader = DataLoader(treino, batch_size=tamanho_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "            annotations_file : str\n",
    "                endere√ßo do CSV das labels das imagens\n",
    "            img_dir : str\n",
    "                endere√ßo do diret√≥rio onde est√£o as imagens\n",
    "            transform : function\n",
    "                fun√ß√µes de transforma√ß√£o que podem ser aplicadas nas imagens\n",
    "            target_transform : function\n",
    "                fun√ß√µes de transforma√ß√£o que podem ser aplicadas nas labels\n",
    "\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) # pega o endere√ßo de uma √∫nica imagem\n",
    "        image = read_image(img_path) # transforma essa imagem em tensor\n",
    "        label = self.img_labels.iloc[idx, 1] # pega a label dessa imagem\n",
    "        # Se houverem transforma√ß√µes a serem apliacadas, aplic√°-las\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Constru√≠ndo as redes neurais üß†\n",
    "\n",
    "Redes neurais s√£o basicamente compostas por diversas camadas, cada uma com um tipo de opera√ß√£o. O m√≥dulo `torch.nn` possui todos os blocos que precisamos para a constru√ß√£o dessas redes. Todas as redes neurias no PyTorch s√£o filhos da classe `nn.Module`, por isso precisamos que nossa rede dependa dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer o processamento em uma GPU caso seja poss√≠vel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Usando {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers mais usados\n",
    "\n",
    "Vamos fazer um exemplo com um minibatch de 3 \"imagens\" feitas de pontos aleat√≥rio e ver como seriam as etapas de cada camada de uma rede neural de maneira individual, para que depois possamos junt√°-los em uma rede s√≥.\n",
    "\n",
    "**Dados de entrada**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.randn((3,28,28))\n",
    "print(f\"A dimens√£o da entrada √© : {input_image.size()}\")\n",
    "\n",
    "# Para mostrar as \"imagens\"\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 1\n",
    "for i in range(1,4):\n",
    "    img = input_image[i-1]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camada de achatamento**\n",
    "\n",
    "A camada de ``nn.Flatten`` converte imagens 2D em um √∫nico vetor. No caso, uma imagem de 28x28 se torna um vetor de 784 elementos, em que cada elmento √© um pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(f\"Dimens√£o das imagens achtadas {flat_image.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camada Linear**\n",
    "\n",
    "A camada linear √© onde aplicamos as opera√ß√µes entre os pesos (weights), bias e dados. Possui esse nome porque essa opera√ß√£o √© uma \"Transforma√ß√£o linear\": $W \\cdot X + b$, repare como esse formato lembra da \"equa√ß√£o linear\" que aprendemos na escola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20) # Aqui definimos qual a dimens√£o da entrada e qual ser√° a dimens√£o da sa√≠da\n",
    "hidden1 = layer1(flat_image) # Passamos nossa imagem achatada para a camada\n",
    "print(f\"Dimens√£o da imagem depois de ter passado pela primeira camada: {hidden1.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camada de Ativa√ß√£o**\n",
    "\n",
    "**A fun√ß√£o de ativa√ß√£o retificadora linear** (*Rectified Linear Activation Function*) - ReL \n",
    "\n",
    "Para conseguirmos passar um sinal para a pr√≥xima camada, necessitamos de fun√ß√µes de ativa√ß√£o. Duas fun√ß√µes comuns s√£o as [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) e [tangente hiperb√≥lica](https://mathworld.wolfram.com/HyperbolicTangent.html), ambas fun√ß√µes n√£o lineares, uma propriedade que ajuda nosso modelo a compreender fun√ß√µes mais complexas. Por√©m, como elas s√£o fun√ß√µes com limites bem estabelecidos, elas acabam \"saturando\" suas sa√≠das, sendo sens√≠veis apenas para seus valores intermedi√°rios. A solu√ß√£o √© utilizar a fun√ß√£o de ativa√ß√£o retificadora linear (ReL) nos *hidden layers*. Dizemos que um n√≥ (ou neur√¥nio) com essa fun√ß√£o de ativa√ß√£o √© uma unidade de ativa√ß√£o retificadora linear (ReLU)\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    x & \\text{se } x > 0, \\\\\n",
    "    0 & \\text{caso contr√°rio}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "![Fun√ß√µes de Ativa√ß√£o](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1200%2F1*ZafDv3VUm60Eh10OeJu1vw.png&f=1&nofb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Antes do ReLU: {hidden1}\\n\\n\")\n",
    "activation1 = nn.ReLU()\n",
    "hidden1 = activation1(hidden1)\n",
    "print(f\"Depois ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juntando todas as camadas sequencialmente**\n",
    "\n",
    "o ``nn.Sequential`` √© uma esp√©cied de container de m√≥dulos. Os dados s√£o passados para ele na mesma ordem que definimos, ele server para simplificar a cria√ß√£o das redes neurais. Vamos criar um exemplo com as camadas que fizemos at√© agora:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28) # batch de 3 imagens de entrada\n",
    "logits = seq_modules(input_image) # Valores de sa√≠da s√£o \"logits\", valores qeu relacionam probabilidades com numeros reais\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun√ß√£o ``nn.Softamx``**\n",
    "\n",
    "Como a √∫ltima camada retorna \"logits\", podemos pass√°-los por uma fun√ß√£o chamada **Softmax** que transforma esses n√∫meros reais em valores de probabilidades (valores entre 0 ou 1). Mostrando qual a categoria mais prov√°vel para aquela imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probabilities = softmax(logits)\n",
    "pred_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A rede neural\n",
    "\n",
    "Agora que voc√™ conhece todos os blocos da rede, podemos junt√°-los para criar a rede neural! Para isso ciramos uma classe que √© inicializada no m√©todo ``__init__`` com os elementos da rede. De maneira similar √†s classes do ``Dataloader``, qualquer filho do m√≥dulo ``nn.Module`` (ou seja, qualquer rede neural que criarmos no PyTorch), precisa de um m√©todo ``forward``, o m√©todo que passa os dados pela rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__() # Herda os atributos do nn.Module do pytorch\n",
    "        self.flatten = ... # Define o m√©todo de achatar imagens\n",
    "        self.nn = ...(   # Inicia o container de camadas\n",
    "            ...,         # camada linear com 784 entradas e 512 sa√≠das\n",
    "            ...,         # unidade de ativa√ß√£o\n",
    "            ...,         # camada linear de 512 entradas e 512 sa√≠das\n",
    "            ...,         # unidade de ativa√ß√£o\n",
    "            ...,         # √∫ltima camada linear com 512 entradas e 10 sa√≠das \n",
    "        )\n",
    "\n",
    "    def forward(self, x):    # m√©todo que passa os dados para a rede neural\n",
    "        x = ...              # achata as imagens para uma dimens√£o\n",
    "        logits = ...         # pasas os dados pela rede, retornando 10 logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede neural\n",
    "\n",
    "Vamos agora para a parte mais importante das redes neurais, trein√°-las! Para isso vamos usar os dados de treino e teste que j√° preparamos antes ``train_dataloader`` e ``test_dataloader``. Vamos tamb√©m inicializar a nossa classe de rede neural no objeto ``model``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperpar√¢metros\n",
    "\n",
    "S√£o par√¢metros que s√£o ajust√°veis e permitem que poss√°mos controlar o modelo. Tipos diferentes de hiperpar√¢metros podem ter grandes efeitos no modelo. No nosso caso, vamos ter 3:\n",
    " - N√∫mero de epochs: n√∫mero de vezes que nosso modelo vai **passar por todo o dataset**\n",
    " - Tamanho do batch: n√∫mero de **amostras do nosso dataset** passadas pro modelo antes de atualizar seus par√¢metros\n",
    " - Learning Rate: O quanto nosso modelo vai **atualizar seus par√¢metros**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fun√ß√£o de perda (loss)\n",
    "\n",
    "Quando nosso modelo est√° aprendendo ele n√£o sabe a resposta certa. A **fun√ß√£o de perda (loss)** √© um meio de medir o qu√£o certo ou errado nosso modelo estava da predi√ß√£o desejada, e √© esse valor que queremos minimizar durante o treinamento (ou seja, que ele tenha o menor erro poss√≠vel). Para computar esse erro, passamos o valor predito por nosso modelo e o valor alvo e calculamos essa diferen√ßa de algum modo.\n",
    "\n",
    "Existem diferentes tipos de calcular o erro do modelo. Algumas fun√ß√µes comuns s√£o [`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Squared Error) para regress√µes, [`nn.NNLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) para classifica√ß√µes e [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) que combina a fun√ß√£o `nn.LogSoftmax` com o `nn.NNLoss`.\n",
    "\n",
    "Para nosso modelo, iremos usar a ``nn.CrossEntropyLoss` j√° que j√° normaliza nossos logits em valores de probabilidade e calcula uma loss comum em tarefas de classifica√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otimizador\n",
    "\n",
    "Com nossa fun√ß√£o de perda definida, temos que definir agora qual t√©cnica utilizaremos para chegar reduzir o erro em cada passo de treino. Existem [diferentes tipos de algoritmos de otimiza√ß√£o](https://pytorch.org/docs/stable/optim.html) para redes neurais, sendo o **SGD** (Stocastic Gradient Descent) o mais simples e o que vamos utilizar para nosso modelo. Alguns men√ß√µes de algoritmos que usamos bastante tamb√©m √© o ADAM e o RMSProp, que podem funcionar melhor dependendo do seu tipo de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ... # Passamos os par√¢metros do nosso modelo e o leraning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando formos fazer nosso loop de treinamento, geralmente o passo de otimiza√ß√£o √© realizado vom 3 passos:\n",
    " - Chamar ``optimizer.zero_grad()`` para resetar os gradientes dos par√¢metros do modelo. Gradientes por padr√£o se somam, para evitar que sejam contados duas vezes em treinamentos diferentes, nos explicitamente zeremos eles a cada itera√ß√£o.\n",
    " - Faz a backpropagation da loss com ```loss.backwards()``. O autograd do PyTorch automaticamente depoisita os gradientes em rela√ß√£o a cada par√¢metro. \n",
    " - Com os gradientes calculados, podemos usar ``optimizer.setp()`` para ajustar os par√¢metros dos gradientes coletados em cada passo anterior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop de otimiza√ß√£o\n",
    "Com nossos hiperpar√¢metros e otimizador prontos, podemos nos preparar par definir o loop de treino. Cada itera√ß√£o desse loop de otimia√ß√£o √© chamado de **epoch**. O epoch consiste de duas partes principais\n",
    "- **O loop de treino**: itera sobre o dataset de treino, tentando convergir os par√¢metros para o melhor poss√≠vel.\n",
    "- **O loop de teste**: itera sobre o dataset de teste para checar como o modelo est√° se saindo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Loop de treino do modelo\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "        dataloader: dataloader do nosso dataset de treino definido anteriormente\n",
    "        model: objeto com o modelo da nossa rede neural\n",
    "        loss_fn: nossa fun√ß√£o de perda\n",
    "        optimizer: o otimizador definido\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset) # pega o tamanho do dataset\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Computa a predi√ß√£o do modelo e a loss\n",
    "        pred = ...\n",
    "        loss = ...\n",
    "\n",
    "        # Backpropagation\n",
    "        ...       # zera os gradientes\n",
    "        ...       # faz a back propagation\n",
    "        ...       # d√° um passo do otimizador\n",
    "\n",
    "        # A cada 100 treinos printamos as m√©tricas\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Loop de teste do modelo\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "        dataloader: dataloader do nosso dataset de teste definido anteriormente\n",
    "        model: objeto com o modelo da nossa rede neural\n",
    "        loss_fn: nossa fun√ß√£o de perda\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Como n√£o vamos otimizar nada, n√£o vamos acompanhar os gradientes\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)  # computa a predi√ß√£o do modelo\n",
    "            test_loss += loss_fn(pred, y).item() # loss dessa predi√ß√£o\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # quantas vezes nosso modelo acertou a predi√ß√£o\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Erros do teste: \\n Acur√°cia: {(100*correct):>0.1f}%, loss m√©dia: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos finalmente treinar nosso modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss, optimizer)\n",
    "    test_loop(test_dataloader, model, loss)\n",
    "print(\"Acabou!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brincando com o modelo\n",
    "Eu fiz uns desenhos toscos e vou ver como o modelo classifica eles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import ImageReadMode\n",
    "\n",
    "custom = []\n",
    "for i in range(1,10):\n",
    "    path = \"./desenhos/cam_\" + str(i) + \".png\"\n",
    "    img = read_image(path, ImageReadMode.GRAY)\n",
    "    custom.append(img)\n",
    "\n",
    "def predict(img):\n",
    "    return labels_map[torch.argmax(model(img/255)).item()]\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, 10):\n",
    "    img = custom[i-1]\n",
    "    label = predict(img)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìùRecomenda√ß√£o de exerc√≠cios\n",
    "- üî¢ Um data set com [n√∫meros desenhados a m√£o](https://www.kaggle.com/c/digit-recognizer) (MNIST cl√°ssico), para treinar os b√°sicos\n",
    "- ü¶ò Um dataset que utiliza csv, [predi√ß√£o de chuvas na australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)\n",
    "- üêü Um dataset de [imagens de peixes](https://www.kaggle.com/crowww/a-large-scale-fish-dataset), para treinar criar datasets/dataloaders\n",
    "- ü¶† Um data set de [Tweets sobre o coronavirus ](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification), para treinar classifica√ß√£o de texto "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe7cbed4bc17059f1642dccf61018811625bb91516f85815cc0235fd6d517ea8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
